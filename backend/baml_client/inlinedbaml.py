# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\n// Using the new OpenAI Responses API for enhanced formatting\n\n// Example Vertex AI client (uncomment to use)\nclient<llm> Gemini {\n  provider google-ai\n  options {\n    model \"gemini-2.0-flash-lite\"\n    api_key env.GEMINI_API_KEY\n  }\n}\n\n\n",
    "ecommerce.baml": "enum ConversationType {\n  GENERAL_CONVERSATION\n  PRODUCT_RECOMMENDATION  \n  IMAGE_SEARCH\n}\n\nclass ProductRecommendation {\n  product_id string\n  title string\n  description string\n  category string\n  price float?\n  rating float?\n  relevance_score float\n  reason string\n}\n\nclass SearchFilters {\n  category string?\n  min_price float?\n  max_price float?\n  min_rating float?\n}\n\n// Unified single-entrypoint helper types\nclass UserQueryInput {\n  user_message string\n  has_image bool\n  image_description string?\n}\n\nclass AgentDirective {\n  intent ConversationType\n  // For GENERAL_CONVERSATION\n  reply string?\n  // For PRODUCT_RECOMMENDATION or IMAGE_SEARCH\n  refined_query string?\n  user_filters string?\n}\n\nfunction HandleGeneralConversation(user_message: string) -> string {\n  client Gemini\n  prompt #\"\n    You are Cartly, a helpful AI shopping assistant for an e-commerce website.\n    \n    User message: \"{{user_message}}\"\n    \n    Respond in a friendly, helpful way. If asked about your capabilities, mention:\n    - I can help you find products by describing what you need\n    - I can search for products based on images you upload\n    - I can have general conversations about shopping and products\n    - I have access to 800 curated products from top e-commerce categories\n    \n    If asked about your name, say you're Cartly, the AI shopping assistant.\n    Keep responses concise and engaging.\n  \"#\n}\n\nfunction AnalyzeProductImage(image_description: string) -> string {\n  client Gemini\n  prompt #\"\n    Based on this image description: \"{{image_description}}\"\n    \n    Generate a search query to find similar or related products in an e-commerce catalog.\n    Focus on:\n    - Product type and category\n    - Key visual features (color, style, material)\n    - Likely use case or purpose\n    - Brand or style characteristics\n    \n    Return a concise search query that would find similar products.\n  \"#\n}\n\n\n// (Removed unused AnalyzeSearchIntent)\n\n// Single entrypoint: take one input and decide what to do.\nfunction HandleUserQuery(input: UserQueryInput) -> AgentDirective {\n  client Gemini\n  prompt #\"\n    {{ ctx.output_format }}\n\n    You are Cartly, an AI shopping assistant. Decide the user's intent and provide\n    either a direct reply (for GENERAL_CONVERSATION) or a refined_query (for\n    PRODUCT_RECOMMENDATION or IMAGE_SEARCH). Use the following rules:\n\n    - If input.has_image is true, set intent = IMAGE_SEARCH. Use input.image_description\n      to craft a concise refined_query for catalog search.\n    - Otherwise, classify input.user_message as one of:\n        GENERAL_CONVERSATION | PRODUCT_RECOMMENDATION\n    - For GENERAL_CONVERSATION: set reply to a concise, helpful answer. Do not\n      request tools.\n    - For PRODUCT_RECOMMENDATION: create a refined_query that captures product type,\n      key attributes, and any implicit constraints. Keep it short.\n\n    Input:\n    message: \"{{ input.user_message }}\"\n    has_image: {{ input.has_image }}\n    image_description: {{ input.image_description }}\n\n  \"#\n}\n\n// (Removed unused ToolSearchByText â€” handled by Python SearchEngine)\n\n// TOOL 2: Image search refinement / explanation helper\nfunction ExplainRecommendation(\n  product: string,\n  user_query: string\n) -> string {\n  client Gemini\n  prompt #\"\n    Explain why this product is a good match for the user's search:\n    \n    User Query: \"{{user_query}}\"\n    Product: {{product}}\n    \n    Provide a personalized explanation that highlights:\n    1. How this product meets their specific needs\n    2. Key features that make it stand out\n    3. Value proposition\n    \n    Keep the explanation conversational and helpful, like a knowledgeable sales assistant.\n  \"#\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.209.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
}

def get_baml_files():
    return _file_map